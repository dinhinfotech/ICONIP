
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{amssymb,amsmath,array}

\usepackage{url}
\urldef{\mailsa}\path|{alfred.hofmann, ursula.barth, ingrid.haas, frank.holzwarth,|
\urldef{\mailsb}\path|anna.kramer, leonie.kunz, christine.reiss, nicole.sator,|
\urldef{\mailsc}\path|erika.siebert-cole, peter.strasser, lncs}@springer.com|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Joint Neighborhood Subgraphs Link Prediction}

% a short form should be given in case it is too long for the running head
%\titlerunning{Lecture Notes in Computer Science: Authors' Instructions}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Dinh Tran-Van$^1$ \and Alessandro Sperduti$^1$ \and Fabrizio Costa$^2$}
%
\authorrunning{Dinh Tran Van, Alessandro Sperduti, and Fabrizio Costa$^2$}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{$^1$ Department of Mathematics, Padova University, Italy\\
%via Trieste, 63, 35121 Padova, Italy\\
$^2$ Department of Computer Science, University of Exeter, United Kingdom\\ 
$\lbrace$dinh, sperduti$\rbrace$@math.unipd.it, f.costa@exeter.ac.uk
%\mailsc\\
}
%\url{http://www.springer.com/lncs}}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

\toctitle{Lecture Notes in Computer Science}
\tocauthor{Authors' Instructions}
\maketitle


\begin{abstract}
Graphs are common data structure used to represent data in which the relation between entities are encoded. A number of graph-based methods and systems have been proposed, varying from biology to recommendation to social network. Most methods have to take partially observed graphs as the input to proceed due to the lack of information. This prevents them from showing promising results. Link prediction becomes an effective paradigm to solve this problem. However, most existing link prediction methods are based on the transitive manner which cannot effectively exploit graph structures. Here we propose a method that first represents each link between two nodes by a graph composed of their neighborhood subgraphs. We then employ a decompositional graph kernel to measure the similarity between links. An empirical experiment on different datasets proves that our proposed method shows the state of the art for link prediction.
\keywords{Link prediction, joint neighborhood subgraphs}
\end{abstract}

\section{Introduction and related work}
We are witnessing a constant increase of the rate at which data is being produced and made available in machine readable formats. Interestingly it is not only the quantity of data that is increasing, but also its complexity, i.e. not only are we measuring a number of attributes or features for each data point, but we are also capturing their mutual relationships, that is, we are considering non independent and identically distributed (non i.i.d.) data. This yields collections that are best represented as graphs or relational data bases and requires a more complex form of analysis. As cursory examples of application domains that are particularly  it is sufficient to mention social networks, where nodes are people and edges encode a type of association such as friendship or co-authorship, bioinformatics, where nodes are proteins and metabolites and edges represent a type of chemical interaction such as catalysis or signaling, and e-commerce, where nodes are people and goods and edges encode a ''buy'' or ''like'' relationship.
A key characteristic of this type of data collections is the sparseness and dynamic nature, i.e. the fact that the number of recorded relations is significantly smaller than the number of all possible pairwise relations, and the fact that these relations evolve in time. A crucial computational task is then the ''link prediction problem'' which allows to suggest friends, or possible collaborators for scientists in social networks, or to discover unknown interactions between proteins to explain the mechanism of a disease in biological networks, or to suggest novel products to be bought to a customer in a e-commerce recommendation system. The many approaches to link prediction that exist in literature can be partitioned according to 1) whether additional or ''side'' information is available for nodes and edges or rather only the network topology is considered and 2) whether the approach is unsupervised or supervised. 

Unsupervised learning methods are non-adaptive, and hence computationally efficient, and define a score for any node pair that is proportional to the existence likelihood of an edge between the two nodes. \textit{Adamic-Adar} \cite{adamic} computes the weighted sum over the common neighbors where the weight is inversely proportional to the (log of) each neighbor node degree. The \textit{preferential attachment} method computes a score simply as the product of the node degrees in an attempt to exploit the 'rich get richer' property of certain network dynamics. \textit{Katz} \cite{katz} takes into account the number of common paths with different lengths between two nodes, assigning more weight to shorter paths. The \textit{Leicht-Holme-Newman} method \cite{lhni} computes the number of intermediate nodes. In \cite{matrix-factorization} the score is derived from the singular value decomposition of the adjacency matrix. 

Supervised link prediction methods convert the problem into a binary classification task where links present in the network (at a given time) are considered as positive instances and a subset of all the non links are considered as negative instances. Following \cite{matrix-factorization}, we can further group these methods into four classes: feature-based models, graph regularization models, latent class models and latent feature models. 
A Bayesian nonparametric approach is used in \cite{nonparametric} to compute a nonparametric latent feature model that does not need a user defined number of latent features but rather induces it as part of the training phase. 
In \cite{matrix-factorization} a matrix factorization approach is used to extract latent features that can take into consideration the output of an arbitrary unsupervised method. The authors show a significant increase in predictive performance when considering a ranking loss function suitable for  the imbalance problem, i.e. when the number of negative is much larger than the number of positive instances. 

In general supervised methods exhibit better accuracies compared to unsupervised methods although incurring in much higher computational and memory complexity costs.
Moreover, most approaches implicitly represent the link prediction problem and the inference used to tackle it as a disjunction over the edges, that is, information on edges is propagated in an additive fashion so that for a node to have $k$ neighbors or $k+1$ does not make a drastic difference.
We claim that this hypothesis is likely putting a cap on the discriminative power of classifiers and therefore we propose a novel supervised method that employs a conjunctive representation. We call the method ''joint neighborhood subgraphs link prediction'' (JNSL). The key idea here is to transform the link prediction task into a binary classification on suitable small subgraphs which we then solve using an efficient graph kernel method.


\section{Method}

\subsection{Definitions and notation}
We represent a problem instance as a graph $G=(V,E)$ where $V$ is the set of nodes and $E$ is the set of links. The set $E$ is partitioned into the subset of observed links ($O$) and the subset of unobserved links ($U$). Like other approaches we assume that all unobserved links are indeed ''non-links'' and we therefore define the link prediction problem as the task of ranking candidate links from the most to the least probable to recover links in $O$ but not in $U$ exploiting only the network topology.


We define the \textit{distance} $\mathcal{D}(u,v)$ between two nodes $u$ and $v$, as the number of edges on the shortest path between them. The \textit{neighborhood} of a node $u$ with radius $r$, $N_r(u) = \lbrace v\ |\ d(u,v) \leq r \rbrace$, is the set of nodes at distance no greater than $r$ from $u$. The corresponding \textit{neighborhood subgraph} $\mathcal{N}_{r}^{u}$ is the  subgraph induced by the neighborhood (i.e. considering all the edges with endpoints in $N_r(u)$). The \textit{degree} of a node $u$, $d(u) = |\mathcal{N}_{1}^{u}|$, is the cardinality of its neighborhood. The maximum node degree in the graph $G$ is $d(G)$.



\subsection{Link encoding as subgraphs union}
Most methods for link prediction compute pairwise nodes similarities treating  the nodes defining the candidate edge independently. Instead we propose to jointly consider both candidate endpoint nodes together with their extended ''context''. To do so we build a graph starting from the two nodes and the underlying network. Given nodes $u$ and $v$, we first extract the two neighborhood sets with a user defined radius $R$ rooted at $u$ and $v$ to obtain $N_R(u)$ and $N_R(v)$, respectively. We then consider the graph $\mathcal{J}$ induced by the set union $N_R(u) \cup N_R(u)$. Finally we add an auxiliary node $w$ and the necessary edges to connect it to $u$ and $v$ (see Fig.\ref{key}).


\subsection{Node labeling}
We propose to use a graph kernel approach to classify the subgraphs encoding each link. In our setup nodes are not endowed with any ''side'' information. However to increase the discriminative power of the similarity notion induced by the graph kernel, instead of assuming a dummy, non-informative label on each node, we propose to use a node labeling function $\ell$ which assigns a discrete label as the node degree. As the degree distribution can in principle extend to large values, we employ a  discretization technique. 
First, we use the degree values for their labels for nodes having degree less than or equal 5. We then discretize the degrees larger than a user defined threshold $T$ ($T=5$ in our experimental evaluation) into $k$ bins and assign the same label for nodes belong to the same bin. We label for nodes owning similar degrees with the same label due to the assumption that nodes with similar degrees tend to share common properties. Formally, the function is defined as:
\begin{center}
$\ell(u) = \left\{
	\begin{array}{ll}
		d(u),\  & \mbox{if } d(u) \leq T \\
		T+i,\ & \mbox{if } d(u) > T
	\end{array}
\right.$,
\end{center}
where $i = \lceil \frac{d(u)-T}{bin}\rceil$, $bin = \frac{d(G)-T}{\lambda - T}$ and $\lambda$ ($\lambda > T$) is the number of labels used to label for nodes in the graphs. The value of $\lambda$ depend on the histogram of graph node degree. We can use tuning technique in order to obtain suitable value for $\lambda$.



\subsection{The neighborhood subgraph pairwise distance kernel}
In this section, we briefly desbribe an efficient kernel named NSPDK \cite{nspdk} which allows to measure the similarity between labeled graphs. NSPDK is an instance of decompositional kernel \cite{convolution-kernel}.

Given a labeled graph $G \in \mathcal{G}$ and two rooted graphs $A_u, B_v$, we first define the relation $R_{r,d}(A_u, B_v, G)$ to be true {\em iff} $A_u \cong \mathcal{N}_r^u$ is (up to isomorphism $\cong$) a neighborhood subgraph with radius $r$ of $G$ and so is $B_v \cong  \mathcal{N}_r^v$, such that $v$ is a distance $d$ from $u$: $\mathcal{D}(u,v)= d$. We then define the inverse relation $R^{-1}$ that returns all pairs of neighborhoods of radius $r$ at distance $d$ in $G$, $R^{-1}_{r,d}(G) = \lbrace A_u, B_v | R_{r,d}(A_u,B_v,G)=true\rbrace$. The kernel $\kappa_{r,d}$ over $\mathcal{G} \times \mathcal{G}$ is the number of such fragments in common in two input graphs:
\begin{center}
$\kappa_{r,d}(G,G^{'}) = 
\!\!\!\!\!\!\!\!\!\!\!\! 
\sum\limits_{\substack{A_u, B_v \ \in \ R_{r,d}^{-1}(G) \\ 
{A'}_{u'}, {B'}_{v'} \ \in \ R_{r,d}^{-1}(G')
}} \!\!\!\!\!\!\!\!\!\!\!\!  { { \textbf{1}_{A_{u} \cong A'_{u'}}} \cdot {
\textbf{1}_{B_{v} \cong B'_{v'}}} }$, 
\end{center}
\noindent where $\textbf{1}_{A \cong B}$ is the \textit{exact matching function} that returns 1 if $A$ is isomorphic to $B$ and 0 otherwise. Finally, the NSPDK is defined as $K(G,G') = \sum\limits_{r}{\sum\limits_{d}{\kappa_{r,d}(G,G')}}$, where for efficiency reasons, the values of $r$ and $d$ are upper bounded to a given maximal $r^*$ and $d^*$, respectively.

Related to exact matching function, this function needs to solve the graph isomorphism problem. This problem is not known to be solvable in polynomial time nor to be NP-complete. In \cite{nspdk}, they propose an approximate method to figure out graph isomorphism problem. It first transforms graphs into strings. It then employ a hash function to map each unique string with an integer number. Finally, two graphs are isomorphic if they have the same corresponding integer numbers.

\subsection{Joint neighborhood subgraphs link prediction}
Given a graph $G(V,E)$ and a set of links $L = \lbrace(u,v)\ |\ (u,v)\in\ O\ or\ (u,v)\in U \rbrace$, our method consists five steps:
\begin{itemize}
	\item Node labeling: Nodes in $G$ are labeled using a defined node label function $\ell$.
	\item Link representing: Link in $L$ are represented as joint neighborhood subgraphs. As a result, we have a set of graphs $SG = \lbrace G_{uv}\ |\ (u,v) \in L \rbrace$
	\item Kernel computation: Obtained graph set $SG$ is used to compute a gram matrix, $K$, by using NSPDK. $K$ shows the similarities between graphs (links).
	\item Model configuration: $K$ is then fed into a kernel machine to build a model which tries to learn a function to separate observed links from non-observed ones in the sense of properties.
	\item Scoring and ranking: Now the model is used to return scores for coming links. These scores are used to rank links from the most to the least probable of being present in the graph.
\end{itemize}

\section{Experiment}
In this section, we intend to measure the performance of our proposed method and compare it with other link prediction methods. We follow the procedure used in \cite{matrix-factorization} which employs six datasets in different domains. In the following we shortly describe each dataset.
\begin{itemize}
\item \textit{Protein }\cite{protein-protein}: the medium confidence network which encodes the interaction between proteins. It contains 2617 nodes and 11855 links. The average degree is 9.1.

\item \textit{Metabolic} \cite{metabolic}: a biological network which relates enzyme proteins and chemical compounds and it includes 668 nodes and 2782 links. The average degree is 8.3.

\item \textit{Nips} \cite{nips}: a network of co-authors at NIPS conference from the first to the $12^{th}$ edition. Nodes are authors and links encode co-author relation between authors. This network contains 2865 nodes and 4733 links. The average degree is 3.3.

\item \textit{Condmat} \cite{condmat}: a network of co-authors for condensed matter physists. This network has 14230 nodes and 1196 links. The average degree is 0.17.

\item \textit{Conflict} \cite{conflict1}, \cite{conflict2}: a network describing the dispute between 130 countries. A link connecting two nodes (countries) if they dispute. We have 180 links in total in this network. The average degree is 2.5.

\item \textit{Powergrid} cite{powergrid}: a network of electric powergrid in US. It has 4941 nodes and 6594 links. The average degree is 2.7.
\end{itemize}

We evaluate the performance of employed methods via a 10 splits. Each round, a given ratio of links will be used to train models, and the rest of links are used to test. For \textit{Protein}, \textit{Metabolic}, \textit{Nips} and \textit{Conflict} networks, we set the training ratio to 10$\%$ while the training ratio for \textit{Condmat} and \textit{Powergrid} to 90$\%$. The performance of each method is computed by taking an average of ROC-AUC over 10 rounds.

\textbf{Model Selection}: The values of different hyper-parameters are set by using a 3-fold on training set in which we use one fold for training and the rest two folds for validation. We tune the values of radius for extracting subgraphs $R$ in $\lbrace 1, 2 \rbrace$, $\lambda$ in node label function in $\lbrace 10, 15 \rbrace$, for $r$ and $d$ parameters of NSPDK in $\lbrace  1, 2 \rbrace$ and $\lbrace  1, 2, 3 \rbrace$, respectively. Finally, the regularization tradeoff $C$ for the SVM is picked up in $\lbrace 10^{-4}, 10^{-3}, 10^{-2},\ 10^{-1}, 1,\ 10,\ 10^2, 10^3,\ 10^4 \rbrace$.

\section{Results and Discussion}
Table \ref{result_table} shows the performance of link prediction methods in ROC-AUC on six datasets in which we mark as bold for the best ROC-AUC on each dataset. From the results on the table, we can group methods into two groups based on their performances: supervised methods and unsupervised methods. The performance of supervised methods are considerably higher than unsuperivsed ones in most cases, except in the Conflict dataset where Sup-Top outperforms Fact+Scores, but with a very small difference. Concerning supervised methods, JNSL outperforms Fact-Scores in all cases. The difference between their performance is small in PowerGrid and Protein datasets with 0.5$\%$ and 0.8$\%$, respectively. And the big gap is in the Condmat dataset with 7.4$\%$. 

\begin{table}
\caption{The ROC-AUC performance on six datasets of different link prediction methods in which AA: Adamic-Adar, PA \cite{pa} preferential Attachment, SHP: Shortest Path, Sup-Top: Liear regression running on unsupervised scores \cite{matrix-factorization}:, SVD \cite{matrix-factorization}:: Singular value decomposition, Fact+Scores \cite{matrix-factorization}: Factorization with unsupervised scores. }
\centering
\setlength{\tabcolsep}{1mm}
\begin{tabular}{|c|c|c|c|c|c|c|}

\hline
         & \multicolumn{6}{c|}{\textbf{Datasets}}\\
 \hline
\textbf{Methods} & Protein & Metabolic & Nips & Condmat & Conflict & PowerGrid\\
	& ($\%$) & ($\%$) & ($\%$) & ($\%$) & ($\%$) & ($\%$)\\
\hline
AA & 56.4$\pm$0.5 & 52.4$\pm$0.5 & 51.2$\pm$0.2 & 56.7$\pm$1.4 & 50.7$\pm$0.8 & 58.9$\pm$0.3\\
PA & 75.0$\pm$0.3 & 52.4$\pm$0.5 & 54.3$\pm$0.5 & 71.6$\pm$2.6 & 54.6$\pm$2.4 & 44.2$\pm$01.0\\
SHP & 72.6$\pm$0.5 & 62.6$\pm$0.4 & 51.7$\pm$0.3 & 67.3$\pm$1.8 & 51.2$\pm$1.4 & 65.9$\pm$1.5\\
Katz & 72.7$\pm$0.5 & 60.8$\pm$0.7 & 51.7$\pm$0.3 & 67.3$\pm$1.7 & 51.2$\pm$1.4 & 65.5$\pm$1.6 \\
Sup-Top & 75.4$\pm$0.3 & 62.8$\pm$0.1 & 54.2$\pm$0.7 & 72.0$\pm$2.0 & 69.5$\pm$7.6 & 70.8$\pm$6.2\\
SVD & 63.5$\pm$0.3 & 53.8$\pm$1.7 & 51.2$\pm$3.1 & 62.9$\pm$5.1 & 54.1$\pm$9.4 & 69.1$\pm$2.6\\
Fact+Scores & 79.3$\pm$0.5 & 69.6$\pm$0.2 & 61.3$\pm$1.9 & 81.2$\pm$2.0 & 68.9$\pm$4.2 & 75.1$\pm$2.0 \\
JNSL & \textbf{80.1$\pm$0.8} & \textbf{72.5$\pm$0.7} & \textbf{62.1$\pm$0.8} & \textbf{88.6$\pm$2.3} & \textbf{72.0$\pm$0.9} & \textbf{75.6$\pm$0.7} \\
 \hline 
\end{tabular}
\label{result_table}
\end{table}
\section{Conclusion and Future Work}
In this paper, we have proposed an effective method for link prediction. Our method takes advantages from the way to represent links as joint neighborhood subgraphs and the employment of an convolution kernel that is able to efficiently exploit the graph structure. The results from the experiment show that our method is the state of the art for graph structured link prediction, especially when the side information are not available.

For the future work, we plan to investigate to propose a method for link prediction which can make use of the variety of information sources associated with graphs.

\begin{thebibliography}{4}

\bibitem{adamic} Adamic, L.A., and Eytan, A.: Friends and neighbors on the web. Social networks 25.3 (2003): 211-230.

\bibitem{katz} Katz, L.: A new status index derived from sociometric analysis. Psychometrika 18.1 (1953): 39-43.

\bibitem{lhni} Leicht, E.A., et al.: Vertex similarity in networks. Physical Review E 73.2 (2006): 026120.

\bibitem{pa}  Barabasi, A.L, Albert, R.: Emergence of Scaling in Random Networks, Science 286 (1999) 509.

\bibitem{matrix-factorization} Menon, A., and Charles, E.: Link prediction via matrix factorization. Machine Learning and Knowledge Discovery in Databases (2011): 437-452.

\bibitem{nonparametric} Miller, K., Michael, et al.: Nonparametric latent feature models for link prediction. Advances in neural information processing systems. 2009.

\bibitem{protein-protein} Von. M.C, et al.: Comparative assessment of large-scale data sets of protein–protein interactions. Nature 417.6887 (2002): 399-403.

\bibitem{metabolic} Yamanishi, Y., et al.: Supervised enzyme network inference from the integration of genomic data and chemical information. Bioinformatics 21.suppl-1 (2005): i468-i477.

\bibitem{nips} Rowies, S.: NIPS dataset (2002), http://www.cs.nyu.edu/~rwoeis/data.html

\bibitem{condmat} Lichtenwalter, R. N., et al.: New perspectives and methods in link prediction. Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2010.

\bibitem{conflict1} Ghosn, F., et al.: The MID3 data set, 1993–2001: Procedures, coding rules, and description. Conflict Management and Peace Science 21.2 (2004): 133-154.

\bibitem{conflict2} Ward, M.D., et al.: Disputes, democracies, and dependencies: A reexamination of the Kantian peace. American Journal of Political Science 51.3 (2007): 583-601.

\bibitem{powergrid} Watts, D.J., and Steven, H.S.: Collective dynamics of ‘small-world’networks. nature 393.6684 (1998): 440-442.

\bibitem{nspdk} Costa, F, and Kurt, D.G.: Fast neighborhood subgraph pairwise distance kernel. Proceedings of the 26th International Conference on Machine Learning. Omnipress, 2010.

\bibitem{convolution-kernel} Haussler, D.: Convolution kernels on discrete structures. Vol. 646. Technical report, Department of Computer Science, University of California at Santa Cruz, 1999.

%\bibitem{jour} Smith, T.F., Waterman, M.S.: Identification of Common Molecular
%Subsequences. J. Mol. Biol. 147, 195--197 (1981)
%
%\bibitem{lncschap} May, P., Ehrlich, H.C., Steinke, T.: ZIB Structure Prediction Pipeline:
%Composing a Complex Biological Workflow through Web Services. In: Nagel,
%W.E., Walter, W.V., Lehner, W. (eds.) Euro-Par 2006. LNCS, vol. 4128,
%pp. 1148--1158. Springer, Heidelberg (2006)
%
%\bibitem{book} Foster, I., Kesselman, C.: The Grid: Blueprint for a New Computing
%Infrastructure. Morgan Kaufmann, San Francisco (1999)
%
%\bibitem{proceeding1} Czajkowski, K., Fitzgerald, S., Foster, I., Kesselman, C.: Grid
%Information Services for Distributed Resource Sharing. In: 10th IEEE
%International Symposium on High Performance Distributed Computing, pp.
%181--184. IEEE Press, New York (2001)
%
%\bibitem{proceeding2} Foster, I., Kesselman, C., Nick, J., Tuecke, S.: The Physiology of the
%Grid: an Open Grid Services Architecture for Distributed Systems
%Integration. Technical report, Global Grid Forum (2002)
%
%\bibitem{url} National Center for Biotechnology Information, \url{http://www.ncbi.nlm.nih.gov}

\end{thebibliography}

\end{document}
